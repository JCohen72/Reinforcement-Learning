{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "GRID_SIZE = 4\n",
    "GAMMA = 0.9999999999 # Practical approximation of gamma = 1.0 to avoid floating-point precision issues and ensure smoother convergence\n",
    "THETA = 1e-4\n",
    "MAX_ITERATIONS = 1000\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "ACTION_PROBS = {a: 1.0 / len(ACTIONS) for a in ACTIONS}  # Equiprobable random policy\n",
    "ITERATIONS_TO_SAVE = [0, 1, 2, 3, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    \"\"\"\n",
    "    Represents the Gridworld environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=GRID_SIZE):\n",
    "        \"\"\"\n",
    "        Initialize the gridworld environment.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
    "        self.terminal_states = [(0, 0), (grid_size - 1, grid_size - 1)]\n",
    "        self.actions = ACTIONS\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        Given a state and an action, return the next state and reward.\n",
    "        \"\"\"\n",
    "        if state in self.terminal_states:\n",
    "            return state, 0  # No reward in terminal states\n",
    "\n",
    "        i, j = state\n",
    "        if action == 'up':\n",
    "            next_state = (max(i - 1, 0), j)\n",
    "        elif action == 'down':\n",
    "            next_state = (min(i + 1, self.grid_size - 1), j)\n",
    "        elif action == 'left':\n",
    "            next_state = (i, max(j - 1, 0))\n",
    "        elif action == 'right':\n",
    "            next_state = (i, min(j + 1, self.grid_size - 1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "\n",
    "        reward = -1  # Standard reward for each move\n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(actions):\n",
    "    \"\"\"\n",
    "    Create an equiprobable random policy.\n",
    "    \"\"\"\n",
    "    action_probs = {action: 1.0 / len(actions) for action in actions}\n",
    "    return action_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_state(env, state, v, action_probs):\n",
    "    \"\"\"\n",
    "    Evaluate the value of a state under the given policy.\n",
    "    \"\"\"\n",
    "    if state in env.terminal_states:\n",
    "        return 0  # Terminal states have a value of 0\n",
    "\n",
    "    v_new = 0\n",
    "    for action in env.actions:\n",
    "        next_state, reward = env.step(state, action)\n",
    "        prob = action_probs[action]\n",
    "        v_new += prob * (reward + GAMMA * v[next_state])\n",
    "    return v_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, action_probs, iterations_to_save=ITERATIONS_TO_SAVE):\n",
    "    \"\"\"\n",
    "    Perform iterative policy evaluation for the given policy.\n",
    "    \"\"\"\n",
    "    grid_size = env.grid_size\n",
    "    v = np.zeros((grid_size, grid_size))\n",
    "    v_dict = {}\n",
    "    iterations = 0\n",
    "\n",
    "    if 0 in iterations_to_save:\n",
    "        v_dict[0] = v.copy()\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        v_old = v.copy()\n",
    "        iterations += 1\n",
    "\n",
    "        for state in env.states:\n",
    "            v_new = evaluate_state(env, state, v_old, action_probs)\n",
    "            delta = max(delta, abs(v_new - v[state]))\n",
    "            v[state] = v_new\n",
    "\n",
    "        if iterations in iterations_to_save:\n",
    "            v_dict[iterations] = v.copy()\n",
    "\n",
    "        if delta < THETA or iterations >= MAX_ITERATIONS:\n",
    "            break  # Convergence achieved or max iterations reached\n",
    "\n",
    "    v_dict['infty'] = v.copy()\n",
    "    return v, v_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_print(v_k, iteration_label):\n",
    "    \"\"\"\n",
    "    Format and print the value function with row and column labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - v_k: The value function at iteration k (2D numpy array).\n",
    "    - iteration_label: A string representing the iteration (e.g., 'k = 0', 'k = ∞').\n",
    "    \"\"\"\n",
    "    grid_size = v_k.shape[0]\n",
    "    # Round to two significant digits\n",
    "    v_rounded = np.round(v_k, decimals=1)\n",
    "    v_significant = np.vectorize(lambda x: np.format_float_positional(x, precision=2, unique=False, fractional=False, trim='k'))\n",
    "    \n",
    "    # Convert the array to a string array with two significant digits\n",
    "    v_str = v_significant(v_rounded.astype(float))\n",
    "    \n",
    "    print(f\"\\nv_k for the random policy at {iteration_label}:\")\n",
    "    \n",
    "    # Print column headers\n",
    "    col_headers = ['Col {}'.format(j) for j in range(grid_size)]\n",
    "    print('       ' + '  '.join(['{:>5}'.format(ch) for ch in col_headers]))\n",
    "    \n",
    "    # Print each row with row labels\n",
    "    for i in range(grid_size):\n",
    "        row_values = '  '.join(['{:>5}'.format(v_str[i, j]) for j in range(grid_size)])\n",
    "        print('Row {:<2}  {}'.format(i, row_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "v_k for the random policy at k = 0:\n",
      "       Col 0  Col 1  Col 2  Col 3\n",
      "Row 0     0.0    0.0    0.0    0.0\n",
      "Row 1     0.0    0.0    0.0    0.0\n",
      "Row 2     0.0    0.0    0.0    0.0\n",
      "Row 3     0.0    0.0    0.0    0.0\n",
      "\n",
      "v_k for the random policy at k = 1:\n",
      "       Col 0  Col 1  Col 2  Col 3\n",
      "Row 0     0.0   -1.0   -1.0   -1.0\n",
      "Row 1    -1.0   -1.0   -1.0   -1.0\n",
      "Row 2    -1.0   -1.0   -1.0   -1.0\n",
      "Row 3    -1.0   -1.0   -1.0    0.0\n",
      "\n",
      "v_k for the random policy at k = 2:\n",
      "       Col 0  Col 1  Col 2  Col 3\n",
      "Row 0     0.0   -1.7   -2.0   -2.0\n",
      "Row 1    -1.7   -2.0   -2.0   -2.0\n",
      "Row 2    -2.0   -2.0   -2.0   -1.7\n",
      "Row 3    -2.0   -2.0   -1.7    0.0\n",
      "\n",
      "v_k for the random policy at k = 3:\n",
      "       Col 0  Col 1  Col 2  Col 3\n",
      "Row 0     0.0   -2.4   -2.9   -3.0\n",
      "Row 1    -2.4   -2.9   -3.0   -2.9\n",
      "Row 2    -2.9   -3.0   -2.9   -2.4\n",
      "Row 3    -3.0   -2.9   -2.4    0.0\n",
      "\n",
      "v_k for the random policy at k = 10:\n",
      "       Col 0  Col 1  Col 2  Col 3\n",
      "Row 0     0.0   -6.1   -8.4   -9.0\n",
      "Row 1    -6.1   -7.7   -8.4   -8.4\n",
      "Row 2    -8.4   -8.4   -7.7   -6.1\n",
      "Row 3    -9.0   -8.4   -6.1    0.0\n",
      "\n",
      "v_k for the random policy at k = ∞:\n",
      "       Col 0  Col 1  Col 2  Col 3\n",
      "Row 0     0.0   -14.   -20.   -22.\n",
      "Row 1    -14.   -18.   -20.   -20.\n",
      "Row 2    -20.   -20.   -18.   -14.\n",
      "Row 3    -22.   -20.   -14.    0.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform policy evaluation and print the value functions.\n",
    "    \"\"\"\n",
    "    env = Gridworld()\n",
    "    action_probs = create_random_policy(env.actions)\n",
    "    v, v_dict = policy_evaluation(env, action_probs)\n",
    "\n",
    "    # Print the value functions at specified iterations\n",
    "    for k in ITERATIONS_TO_SAVE + ['infty']:\n",
    "        iteration_label = f'k = {k}' if k != 'infty' else 'k = ∞'\n",
    "        v_k = v_dict[k]\n",
    "        format_and_print(v_k, iteration_label)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
