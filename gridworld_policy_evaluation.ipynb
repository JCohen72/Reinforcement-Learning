{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "GRID_SIZE = 5\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE\n",
    "NUM_ACTIONS = 4  # North, South, East, West\n",
    "GAMMA = 0.9  # Discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribution(n, temp=1.0):\n",
    "    \"\"\"\n",
    "    Create a probability distribution over n elements.\n",
    "    \"\"\"\n",
    "    logits = np.random.randn(n)\n",
    "    exp_logits = np.exp(logits / temp)\n",
    "    prob = exp_logits / exp_logits.sum()\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MDP():\n",
    "    \"\"\"\n",
    "    Create the Markov Decision Process (MDP) for the Gridworld.\n",
    "    Returns:\n",
    "        P: Transition probability matrix of shape (NUM_STATES, NUM_ACTIONS, NUM_STATES)\n",
    "        R: Reward matrix of shape (NUM_STATES, NUM_ACTIONS)\n",
    "    \"\"\"\n",
    "    P = np.zeros((NUM_STATES, NUM_ACTIONS, NUM_STATES))\n",
    "    R = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "    \n",
    "    # Special states and their corresponding transitions\n",
    "    special_states = {\n",
    "        1: {'next_state': 21, 'reward': 10},  # State A\n",
    "        3: {'next_state': 13, 'reward': 5}    # State B\n",
    "    }\n",
    "    \n",
    "    # Action effects: North, South, East, West\n",
    "    action_effects = {\n",
    "        0: (-1, 0),  # North\n",
    "        1: (1, 0),   # South\n",
    "        2: (0, 1),   # East\n",
    "        3: (0, -1)   # West\n",
    "    }\n",
    "\n",
    "    for s in range(NUM_STATES):\n",
    "        # Check if the state is a special state (A or B)\n",
    "        if s in special_states:\n",
    "            next_state = special_states[s]['next_state']\n",
    "            reward = special_states[s]['reward']\n",
    "            for a in range(NUM_ACTIONS):\n",
    "                P[s, a, next_state] = 1.0\n",
    "                R[s, a] = reward\n",
    "        else:\n",
    "            row, col = divmod(s, GRID_SIZE)\n",
    "            for a in range(NUM_ACTIONS):\n",
    "                delta_row, delta_col = action_effects[a]\n",
    "                new_row = row + delta_row\n",
    "                new_col = col + delta_col\n",
    "                # Check if the new position is within grid boundaries\n",
    "                if 0 <= new_row < GRID_SIZE and 0 <= new_col < GRID_SIZE:\n",
    "                    next_state = new_row * GRID_SIZE + new_col\n",
    "                    reward = 0  # Standard move reward\n",
    "                else:\n",
    "                    next_state = s  # Agent stays in the same state\n",
    "                    reward = -1  # Penalty for hitting the wall\n",
    "                P[s, a, next_state] = 1.0\n",
    "                R[s, a] = reward\n",
    "    return P, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_policy():\n",
    "    \"\"\"\n",
    "    Create the equiprobable random policy.\n",
    "    Returns:\n",
    "        pi: Policy matrix of shape (NUM_STATES, NUM_ACTIONS)\n",
    "    \"\"\"\n",
    "    pi = np.full((NUM_STATES, NUM_ACTIONS), 1.0 / NUM_ACTIONS)\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, R, pi, gamma):\n",
    "    \"\"\"\n",
    "    Evaluate the given policy in the MDP.\n",
    "    Args:\n",
    "        P: Transition probability matrix\n",
    "        R: Reward matrix\n",
    "        pi: Policy matrix\n",
    "        gamma: Discount factor\n",
    "    Returns:\n",
    "        v: State-value function vector of length NUM_STATES\n",
    "    \"\"\"\n",
    "    # Compute the state transition matrix and reward vector under policy pi\n",
    "    P_pi = np.einsum('sa,san->sn', pi, P)\n",
    "    R_pi = np.einsum('sa,sa->s', pi, R)\n",
    "\n",
    "    # Solve the Bellman equation: v = R_pi + gamma * P_pi * v\n",
    "    # Rearranged to (I - gamma * P_pi) * v = R_pi\n",
    "    I = np.eye(NUM_STATES)\n",
    "    A = I - gamma * P_pi\n",
    "    b = R_pi\n",
    "\n",
    "    # Solve for v using linear system solver\n",
    "    v = np.linalg.solve(A, b)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, R, gamma):\n",
    "    \"\"\"\n",
    "    Perform policy iteration to find the optimal policy.\n",
    "    Args:\n",
    "        P: Transition probability matrix\n",
    "        R: Reward matrix\n",
    "        gamma: Discount factor\n",
    "    Returns:\n",
    "        pi: Optimal policy matrix\n",
    "        v: State-value function for the optimal policy\n",
    "    \"\"\"\n",
    "    # Initialize policy arbitrarily\n",
    "    pi = initial_policy()\n",
    "    is_policy_stable = False\n",
    "\n",
    "    while not is_policy_stable:\n",
    "        # Policy Evaluation\n",
    "        v = policy_evaluation(P, R, pi, gamma)\n",
    "        is_policy_stable = True\n",
    "\n",
    "        # Policy Improvement\n",
    "        for s in range(NUM_STATES):\n",
    "            old_action = np.argmax(pi[s])\n",
    "            q_sa = np.zeros(NUM_ACTIONS)\n",
    "            for a in range(NUM_ACTIONS):\n",
    "                q_sa[a] = R[s, a] + gamma * np.dot(P[s, a], v)\n",
    "            new_action = np.argmax(q_sa)\n",
    "            if old_action != new_action:\n",
    "                is_policy_stable = False\n",
    "            # Update the policy to be greedy w.r.t. q_sa\n",
    "            pi[s] = np.eye(NUM_ACTIONS)[new_action]\n",
    "    return pi, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Value Function for the Equiprobable Random Policy:\n",
      "       Col 0  Col 1  Col 2  Col 3  Col 4\n",
      "Row 0    3.3    8.8    4.4    5.3    1.5\n",
      "Row 1    1.5    3.0    2.3    1.9    0.5\n",
      "Row 2    0.1    0.7    0.7    0.4   -0.4\n",
      "Row 3   -1.0   -0.4   -0.4   -0.6   -1.2\n",
      "Row 4   -1.9   -1.3   -1.2   -1.4   -2.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Construct the MDP\n",
    "    P, R = create_MDP()\n",
    "    # Define the equiprobable random policy\n",
    "    pi = initial_policy()\n",
    "    # Evaluate the policy\n",
    "    v = policy_evaluation(P, R, pi, GAMMA)\n",
    "    # Reshape the value function for display purposes\n",
    "    v_grid = v.reshape((GRID_SIZE, GRID_SIZE))\n",
    "    # Create a pandas DataFrame for better display\n",
    "    df = pd.DataFrame(np.round(v_grid, 1))\n",
    "    df.index = [f\"Row {i}\" for i in range(GRID_SIZE)]\n",
    "    df.columns = [f\"Col {j}\" for j in range(GRID_SIZE)]\n",
    "    print(\"State-Value Function for the Equiprobable Random Policy:\")\n",
    "    print(df.to_string())\n",
    "\n",
    "    # Optionally perform policy iteration to find the optimal policy\n",
    "    # pi_opt, v_opt = policy_iteration(P, R, GAMMA)\n",
    "    # v_opt_grid = v_opt.reshape((GRID_SIZE, GRID_SIZE))\n",
    "    # df_opt = pd.DataFrame(np.round(v_opt_grid, 1))\n",
    "    # df_opt.index = [f\"Row {i}\" for i in range(GRID_SIZE)]\n",
    "    # df_opt.columns = [f\"Col {j}\" for j in range(GRID_SIZE)]\n",
    "    # print(\"\\nOptimal State-Value Function after Policy Iteration:\")\n",
    "    # print(df_opt.to_string())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
